\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\addvspace {10pt}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\addvspace {10pt}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {chapter}{\chapternumberline {3}Introduction to tools of supervised machine learning}{27}{chapter.3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.1}Supervised machine learning to predict and to learn}{27}{section.3.1}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces We can imagine an unknown target function $f$ in nature, that generates the $N$ observed training examples $X_i$ $y_i$ for $i \in 1,2,...,N$. With a hypothesis set $H$, that is the set of hyper parameters we will try for $A$. Each hypothesis will yield a model. With model selection one final hypothesis and the resulting model $g$ is chosen. The model $g$ is a function that mimics $f$. With $g$ we can start to predict how $f$ behaves (direct motive), or secondary evaluate the structure of $g$ to both challenge the validity of $g$ and our current beliefs of $f$. The figure is copied from a set of Caltech lecture slides \cite {Mostafa13learning}. }}{28}{figure.3.1}}
\newlabel{modelPredictExplain}{{\M@TitleReference {3.1}{We can imagine an unknown target function $f$ in nature, that generates the $N$ observed training examples $X_i$ $y_i$ for $i \in 1,2,...,N$. With a hypothesis set $H$, that is the set of hyper parameters we will try for $A$. Each hypothesis will yield a model. With model selection one final hypothesis and the resulting model $g$ is chosen. The model $g$ is a function that mimics $f$. With $g$ we can start to predict how $f$ behaves (direct motive), or secondary evaluate the structure of $g$ to both challenge the validity of $g$ and our current beliefs of $f$. The figure is copied from a set of Caltech lecture slides \cite {Mostafa13learning}. }}{28}{We can imagine an unknown target function $f$ in nature, that generates the $N$ observed training examples $X_i$ $y_i$ for $i \in 1,2,...,N$. With a hypothesis set $H$, that is the set of hyper parameters we will try for $A$. Each hypothesis will yield a model. With model selection one final hypothesis and the resulting model $g$ is chosen. The model $g$ is a function that mimics $f$. With $g$ we can start to predict how $f$ behaves (direct motive), or secondary evaluate the structure of $g$ to both challenge the validity of $g$ and our current beliefs of $f$. The figure is copied from a set of Caltech lecture slides \cite {Mostafa13learning}. }{figure.3.1}{}}
\newlabel{modelPredictExplain@cref}{{[figure][1][3]3.1}{28}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Univariate Regression}{29}{subsection.3.1.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Multiple linear regression and interaction terms}{29}{subsection.3.1.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.2}Cross-validation}{31}{section.3.2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Segregation}{31}{subsection.3.2.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Unbiased prediction error estimation}{32}{subsection.3.2.2}}
\newlabel{unbiased_estimation}{{\M@TitleReference {3.2.2}{Unbiased prediction error estimation}}{32}{Unbiased prediction error estimation}{subsection.3.2.2}{}}
\newlabel{unbiased_estimation@cref}{{[subsection][2][3,2]3.2.2}{32}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Independent and identical sampling}{32}{subsection.3.2.3}}
\newlabel{iid}{{\M@TitleReference {3.2.3}{Independent and identical sampling}}{32}{Independent and identical sampling}{subsection.3.2.3}{}}
\newlabel{iid@cref}{{[subsection][3][3,2]3.2.3}{32}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Transient or constant underlying systems}{33}{subsection.3.2.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Defining training error and prediction}{33}{subsection.3.2.5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Other cross validation regimes}{34}{subsection.3.2.6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.3}Algorithmic models}{35}{section.3.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Left plot, a training set has been simulated from $y = sin(x1 \pi ) − \frac  1 2 x_2^2$ where $x_1$ and $x_2$ are drawn from an uniform distribution within $[-3;3]$. Middle and left plot, RF and RBF-SVM have respectively been trained on the data set, providing two completely similar model structures within the proximity of training data. The the shape of the RF and RBF-SVM model structures heavily disagree outside training set area.}}{36}{figure.3.2}}
\newlabel{svmVSrf}{{\M@TitleReference {3.2}{Left plot, a training set has been simulated from $y = sin(x1 \pi ) − \frac  1 2 x_2^2$ where $x_1$ and $x_2$ are drawn from an uniform distribution within $[-3;3]$. Middle and left plot, RF and RBF-SVM have respectively been trained on the data set, providing two completely similar model structures within the proximity of training data. The the shape of the RF and RBF-SVM model structures heavily disagree outside training set area.}}{36}{Left plot, a training set has been simulated from $y = sin(x1 \pi ) − \frac 1 2 x_2^2$ where $x_1$ and $x_2$ are drawn from an uniform distribution within $[-3;3]$. Middle and left plot, RF and RBF-SVM have respectively been trained on the data set, providing two completely similar model structures within the proximity of training data. The the shape of the RF and RBF-SVM model structures heavily disagree outside training set area}{figure.3.2}{}}
\newlabel{svmVSrf@cref}{{[figure][2][3]3.2}{36}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3.4}random forest}{37}{section.3.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}bagging}{37}{subsection.3.4.1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Introduction to decision trees}{37}{subsection.3.4.2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Two representations of the same model tree. Left, a geometrical representation of the model surface. Right, a graph representation of the same tree model. Nodes are enumerated as created by the learning algorithm. $\mathaccentV {hat}05E{y}^{''}_j$ are the node predictions. $n$ is the node size. $X_1$ and $X_2$ axes are two numerical variables and $y$ axis the prediction axis. This figure is copied and modified from \cite {welling2016forest}.}}{39}{figure.3.3}}
\newlabel{simpleTree}{{\M@TitleReference {3.3}{Two representations of the same model tree. Left, a geometrical representation of the model surface. Right, a graph representation of the same tree model. Nodes are enumerated as created by the learning algorithm. $\mathaccentV {hat}05E{y}^{''}_j$ are the node predictions. $n$ is the node size. $X_1$ and $X_2$ axes are two numerical variables and $y$ axis the prediction axis. This figure is copied and modified from \cite {welling2016forest}.}}{39}{Two representations of the same model tree. Left, a geometrical representation of the model surface. Right, a graph representation of the same tree model. Nodes are enumerated as created by the learning algorithm. $\hat {y}^{''}_j$ are the node predictions. $n$ is the node size. $X_1$ and $X_2$ axes are two numerical variables and $y$ axis the prediction axis. This figure is copied and modified from \cite {welling2016forest}}{figure.3.3}{}}
\newlabel{simpleTree@cref}{{[figure][3][3]3.3}{39}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Regularization in random forest}{39}{subsection.3.4.3}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Single regression tree fit to 2500 data points without noise. $y=(X_1^2 + X_2^3)^2 - (X_1^2 + X_2^3)$, where $X_1$ and $X_2$ are drawn from uniform distributions.}}{40}{figure.3.4}}
\newlabel{decisionTreeFit}{{\M@TitleReference {3.4}{Single regression tree fit to 2500 data points without noise. $y=(X_1^2 + X_2^3)^2 - (X_1^2 + X_2^3)$, where $X_1$ and $X_2$ are drawn from uniform distributions.}}{40}{Single regression tree fit to 2500 data points without noise. $y=(X_1^2 + X_2^3)^2 - (X_1^2 + X_2^3)$, where $X_1$ and $X_2$ are drawn from uniform distributions}{figure.3.4}{}}
\newlabel{decisionTreeFit@cref}{{[figure][4][3]3.4}{40}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Random subspace regularization}{41}{subsection.3.4.4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Choice of regularization}{41}{subsection.3.4.5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Single regression tree fit to 2500 data points with a 33\% unexplainable noise/variance component, $y=(X_1^2 + X_2^3)^2 - (X_1^2 + X_2^3)$, where $X_1$ and $X_2$ are drawn from uniform distributions. (a) single tree, min node size is 5 , (b) single tree, min node size is 150, (c) ensemble of 500 trees, min node size is 5, (d) ensemble 1500 trees, min node size is 50, (e) ensemble of 1500 trees, min node size is 5, bootstrap sample size is 250}}{42}{figure.3.5}}
\newlabel{allTrees}{{\M@TitleReference {3.5}{Single regression tree fit to 2500 data points with a 33\% unexplainable noise/variance component, $y=(X_1^2 + X_2^3)^2 - (X_1^2 + X_2^3)$, where $X_1$ and $X_2$ are drawn from uniform distributions. (a) single tree, min node size is 5 , (b) single tree, min node size is 150, (c) ensemble of 500 trees, min node size is 5, (d) ensemble 1500 trees, min node size is 50, (e) ensemble of 1500 trees, min node size is 5, bootstrap sample size is 250}}{42}{Single regression tree fit to 2500 data points with a 33\% unexplainable noise/variance component, $y=(X_1^2 + X_2^3)^2 - (X_1^2 + X_2^3)$, where $X_1$ and $X_2$ are drawn from uniform distributions. (a) single tree, min node size is 5 , (b) single tree, min node size is 150, (c) ensemble of 500 trees, min node size is 5, (d) ensemble 1500 trees, min node size is 50, (e) ensemble of 1500 trees, min node size is 5, bootstrap sample size is 250}{figure.3.5}{}}
\newlabel{allTrees@cref}{{[figure][5][3]3.5}{42}}
\@setckpt{chapters/introStat}{
\setcounter{page}{44}
\setcounter{equation}{2}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{nag@c}{48}
\setcounter{@memmarkcntra}{0}
\setcounter{storedpagenumber}{1}
\setcounter{book}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{4}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{vslineno}{0}
\setcounter{poemline}{0}
\setcounter{modulo@vs}{0}
\setcounter{memfvsline}{0}
\setcounter{verse}{0}
\setcounter{chrsinstr}{0}
\setcounter{poem}{0}
\setcounter{newflo@tctr}{4}
\setcounter{@contsubnum}{0}
\setcounter{maxsecnumdepth}{3}
\setcounter{sidefootnote}{0}
\setcounter{pagenote}{0}
\setcounter{pagenoteshadow}{0}
\setcounter{memfbvline}{0}
\setcounter{bvlinectr}{0}
\setcounter{cp@cntr}{0}
\setcounter{ism@mctr}{0}
\setcounter{xsm@mctr}{0}
\setcounter{csm@mctr}{0}
\setcounter{ksm@mctr}{0}
\setcounter{xksm@mctr}{0}
\setcounter{cksm@mctr}{0}
\setcounter{msm@mctr}{0}
\setcounter{xmsm@mctr}{0}
\setcounter{cmsm@mctr}{0}
\setcounter{bsm@mctr}{0}
\setcounter{workm@mctr}{0}
\setcounter{sheetsequence}{56}
\setcounter{lastsheet}{224}
\setcounter{lastpage}{212}
\setcounter{figure}{5}
\setcounter{lofdepth}{1}
\setcounter{table}{0}
\setcounter{lotdepth}{1}
\setcounter{parentequation}{0}
\setcounter{lstnumber}{1}
\setcounter{Item}{3}
\setcounter{Hfootnote}{1}
\setcounter{memhycontfloat}{0}
\setcounter{Hpagenote}{0}
\setcounter{Hy@AnnotLevel}{0}
\setcounter{bookmark@seq@number}{37}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{76}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextrayear}{0}
\setcounter{maxextraalpha}{3}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{todocounter}{0}
\setcounter{todocounter@totc}{0}
\setcounter{AM@survey}{0}
\setcounter{blindtext}{1}
\setcounter{Blindtext}{5}
\setcounter{blind@countparstart}{0}
\setcounter{blindlist}{0}
\setcounter{blindlistlevel}{0}
\setcounter{blindlist@level}{0}
\setcounter{blind@listcount}{0}
\setcounter{blind@levelcount}{0}
\setcounter{blind@randomcount}{0}
\setcounter{blind@randommax}{0}
\setcounter{blind@pangramcount}{0}
\setcounter{blind@pangrammax}{0}
\setcounter{float@type}{16}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{2}
}
